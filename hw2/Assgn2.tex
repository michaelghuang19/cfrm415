\documentclass{article}
\linespread{1.3}
\usepackage[margin=50pt]{geometry}
\usepackage{amsmath, amsthm, amssymb, amsthm, tikz, fancyhdr}
\pagestyle{fancy}
\renewcommand{\headrulewidth}{0pt}
\newcommand{\changefont}{\fontsize{15}{15}\selectfont}

\fancypagestyle{firstpageheader}
{
  \fancyhead[R]{\changefont Michael Huang \\ CFRM 415 \\ Homework 2}
}

\begin{document}

\thispagestyle{firstpageheader}

\section*{1.}
{\Large 

We optimize using \\
$\underset{\omega_1, \omega_2, \omega_3}{\text{min}} \frac{1}{2}\omega^T \Sigma \omega$ \\
s.t. $\omega_1 + \omega_2 + \omega_3 = 1$, or $\omega^T 1 = 1$ \\ \\
We know that to the Langrangian will be in the form $\mathcal{L} (\omega, \lambda) = f(x) + \lambda g(x)$, for $g(x) = 0$. We have $f(x) = \frac{1}{2}\omega^T \Sigma \omega$ and $g(x) = \omega^T 1 = 1$, so we can effectively modify this equation to be
$\mathcal{L} (\omega, \lambda) = \frac{1}{2}\omega^T \Sigma \omega + \lambda (\omega^T 1 - 1)$ \\ \\
% for which we take the gradient to get \\
% $\nabla_\omega \mathcal{L} (\omega, \lambda) = \Sigma \omega + \lambda 1 = 0$
In matrix form, we can this as follows: \\
$\frac{1}{2}$
$\cdot$
$\begin{bmatrix}
  \omega_1 & \omega_2 & \omega_3
\end{bmatrix}$ 
$\cdot$
$\begin{bmatrix}
  \sigma_1^2 & \sigma_{12} & \sigma_{13} \\
  \sigma_{12} & \sigma_2^2 & \sigma_{23} \\
  \sigma_{13} & \sigma_{23} & \sigma_3^2 \\
\end{bmatrix}$
$\cdot$
$\begin{bmatrix}
  \omega_1 \\
  \omega_2 \\
  \omega_3 \\
\end{bmatrix}$ 
$+ \lambda \cdot$ 
$(\begin{bmatrix}
  \omega_1 & \omega_2 & \omega_3
\end{bmatrix}
\cdot 
\begin{bmatrix}
  1 \\
  1 \\
  1 \\
\end{bmatrix}
- 1)$ \\ \\
We can then take the gradient with respect to $\omega$ and set to 0: \\ \\
$\nabla_\omega$ $\frac{1}{2}$
$\cdot$
$\begin{bmatrix}
  \omega_1 & \omega_2 & \omega_3
\end{bmatrix}$ 
$\cdot$
$\begin{bmatrix}
  \sigma_1^2 & \sigma_{12} & \sigma_{13} \\
  \sigma_{12} & \sigma_2^2 & \sigma_{23} \\
  \sigma_{13} & \sigma_{23} & \sigma_3^2 \\
\end{bmatrix}$
$\cdot$
$\begin{bmatrix}
  \omega_1 \\
  \omega_2 \\
  \omega_3 \\
\end{bmatrix}$ 
$+ \lambda \cdot$ 
$(\begin{bmatrix}
  \omega_1 & \omega_2 & \omega_3
\end{bmatrix}
\cdot 
\begin{bmatrix}
  1 \\
  1 \\
  1 \\
\end{bmatrix}
- 1)$ \\ \\
$0 = \frac{1}{2}$
$\cdot $
$\begin{bmatrix}
  1 & 0 & 0 \\
  0 & 1 & 0 \\
  0 & 0 & 1 \\
\end{bmatrix}$ 
$\cdot $
$\begin{bmatrix}
  \sigma_1^2 & \sigma_{12} & \sigma_{13} \\
  \sigma_{12} & \sigma_2^2 & \sigma_{23} \\
  \sigma_{13} & \sigma_{23} & \sigma_3^2 \\
\end{bmatrix}$
$\cdot$
$\begin{bmatrix}
  \omega_1 \\
  \omega_2 \\
  \omega_3 \\
\end{bmatrix}$
$+$
$\frac{1}{2}$
$\cdot$
$\begin{bmatrix}
  \omega_1 & \omega_2 & \omega_3
\end{bmatrix}$ 
$\cdot$
$\begin{bmatrix}
  \sigma_1^2 & \sigma_{12} & \sigma_{13} \\
  \sigma_{12} & \sigma_2^2 & \sigma_{23} \\
  \sigma_{13} & \sigma_{23} & \sigma_3^2 \\
\end{bmatrix} $
$\cdot $
$\begin{bmatrix}
  1 & 0 & 0 \\
  0 & 1 & 0 \\
  0 & 0 & 1 \\
\end{bmatrix}$ 
$+ \lambda \cdot$ 
$(\begin{bmatrix}
  1 & 0 & 0 \\
  0 & 1 & 0 \\
  0 & 0 & 1 \\
\end{bmatrix}
\cdot 
\begin{bmatrix}
  1 \\
  1 \\
  1 \\
\end{bmatrix} + 0)$ \\ \\
$0 = \frac{1}{2}$
$\cdot $
$\begin{bmatrix}
  \sigma_1^2 & \sigma_{12} & \sigma_{13} \\
  \sigma_{12} & \sigma_2^2 & \sigma_{23} \\
  \sigma_{13} & \sigma_{23} & \sigma_3^2 \\
\end{bmatrix}$
$\cdot$
$\begin{bmatrix}
  \omega_1 \\
  \omega_2 \\
  \omega_3 \\
\end{bmatrix}$
$+$
$\frac{1}{2}$
$\cdot$
$\begin{bmatrix}
  \omega_1 & \omega_2 & \omega_3
\end{bmatrix}$ 
$\cdot$
$\begin{bmatrix}
  \sigma_1^2 & \sigma_{12} & \sigma_{13} \\
  \sigma_{12} & \sigma_2^2 & \sigma_{23} \\
  \sigma_{13} & \sigma_{23} & \sigma_3^2 \\
\end{bmatrix} $
$+ \lambda \cdot
\begin{bmatrix}
  1 \\
  1 \\
  1 \\
\end{bmatrix}$ \\ \\
$0 = \frac{1}{2}$
$\cdot $
$\begin{bmatrix}
  \omega_1 \sigma_1^2 + \omega_2 \sigma_{12} + \omega_3 \sigma_{13} & \omega_1 \sigma_{12} + \omega_2 \sigma_2^2 + \omega_3 \sigma_{23} & \omega_1 \sigma_{13} + \omega_2 \sigma_{23} + \omega_3 \sigma_3^2
\end{bmatrix}$
$+$ \\ 
$\frac{1}{2}$
$\cdot$
$\begin{bmatrix}
  \omega_1 \sigma_1^2 + \omega_2 \sigma_{12} + \omega_3 \sigma_{13} & \omega_1 \sigma_{12} + \omega_2 \sigma_2^2 + \omega_3 \sigma_{23} & \omega_1 \sigma_{13} + \omega_2 \sigma_{23} + \omega_3 \sigma_3^2
\end{bmatrix}$
$+ \lambda \cdot
\begin{bmatrix}
  1 \\
  1 \\
  1 \\
\end{bmatrix}$ \\ \\
$0 =$
$\begin{bmatrix}
  \omega_1 \sigma_1^2 + \omega_2 \sigma_{12} + \omega_3 \sigma_{13} & \omega_1 \sigma_{12} + \omega_2 \sigma_2^2 + \omega_3 \sigma_{23} & \omega_1 \sigma_{13} + \omega_2 \sigma_{23} + \omega_3 \sigma_3^2
\end{bmatrix}$
$+ \lambda \cdot
\begin{bmatrix}
  1 \\
  1 \\
  1 \\
\end{bmatrix}$ \\
$0 =$
$\begin{bmatrix}
  \sigma_1^2 & \sigma_{12} & \sigma_{13} \\
  \sigma_{12} & \sigma_2^2 & \sigma_{23} \\
  \sigma_{13} & \sigma_{23} & \sigma_3^2 \\
\end{bmatrix}$
$\cdot$
$\begin{bmatrix}
  \omega_1 \\
  \omega_2 \\
  \omega_3 \\
\end{bmatrix}$
$+ \lambda \cdot
\begin{bmatrix}
  1 \\
  1 \\
  1 \\
\end{bmatrix}$ \\ \\
$0 = \Sigma \omega - \lambda 1$ \\
as we sought to show.

}

\section*{2.}
{\Large

We found in problem 1 that the matrix form for the optimizer is $\Sigma \omega + \lambda 1 = 0$. In other words, \\
$\Sigma \omega = -\lambda1$ \\
$\omega = -\lambda\Sigma^{-1}1$ \\
$1^T\omega = -\lambda1^T\Sigma^{-1}1$ \\
$\omega^T1 = -\lambda1^T\Sigma^{-1}1$ \\
$1 = -\lambda1^T\Sigma^{-1}1$ \\
$-\frac{1}{1^T\Sigma^{-1}1} = \lambda$ \\ 
We can now re-susbstitute to find that \\ 
$\omega = -\lambda\Sigma^{-1}1$ \\
$\omega = -\frac{1}{1^T\Sigma^{-1}1} \cdot -\Sigma^{-1}1$ \\ 
$\omega = \frac{\Sigma^{-1}1}{1^T\Sigma^{-1}1}$, and due to the nature of the optimizer, we now have that \\
$\omega^* = \frac{\Sigma^{-1}1}{1^T\Sigma^{-1}1}$ 
% Our previous constraint was that $\omega^T 1 = 1$, which we can now re-form as \\
% $(-\lambda \Sigma^{-1} 1)^T 1 = 1$ \\
% $-\lambda 1^T {\Sigma^{-1}}^T 1 = 1$ \\
% $-\lambda = \frac{1}{1^T {\Sigma^{-1}}^T 1}$ \\
% $\lambda = -\frac{1}{1^T {\Sigma^{-1}}^T 1}$ \\ \\
% And plugging this back in, we can find that \\
% $\omega = -\Sigma^{-1}1 \cdot -\frac{1}{1^T {\Sigma^{-1}}^T 1}$ \\
% $\omega = -\frac{-\Sigma^{-1}1}{1^T {\Sigma^{-1}}^T 1}$ \\
% $\omega = \frac{\Sigma^{-1}1}{1^T {\Sigma^{-1}}^T 1}$ \\
as we hoped to show.

}

\section*{3.}
{\Large 

You are given the market term structure of interest rates as of today, date $t$, and you are about to place a trade for a receiver interest rate swap that will have its first reset date and time $T_0 > t$, with both fixed and floating rates paid at successive dates $T_1, T_2, \dots, T_n$, where the floating rate to be paid at each date $T_i$ is fixed on the previous date $T_{i-1}$. The fixed rate to be paid is $S$, $\tau_i = \tau(T_{i-1}, T_i)$ is the date count-adjusted year fraction on each interval, and the forward interest rate over each interval as seen today is represented by $F(t; T_{i-1}, T_i)$, as we discussed in class.

\subsection*{(a)}

We know that the present value of the fixed leg over a period of $n$ payments is $S\sum_{i=1}^{n} \tau_i P(t, T_i)$ where we use $t$, instead of $T_0$ due to the nature of our trade which starts before the start date. 

\subsection*{(b)}

We know that the present value of the floating leg over a period of $n$ payments is $\sum_{i=1}^{n} \tau_i P(t, T_i)F(t; T_{i-1}, T_i)$ where we use $t$, instead of $T_0$ due to the nature of our trade which starts before the start date. 

\subsection*{(c)}

Using the hint in context tells us that $P(T_{i-1}, T_i) = P(0, T_i) / P(0, T_{i-1}) = P(t, T_i) / P(t, T_{i-1})$. We know that the present value of each leg must be equal at the start time. In other words, \\
$S\sum_{i=1}^{n} \tau_i P(t, T_i) = \sum_{i=1}^{n} \tau_i P(t, T_i)F(t; T_{i-1}, T_i)$ \\ 
$S\sum_{i=1}^{n} P(t, T_i) = \sum_{i=1}^{n} P(t, T_i)F(t; T_{i-1}, T_i)$ \\ 
$S = \sum_{i=1}^{n} F(t; T_{i-1}, T_i)$ \\ 
$S = \sum_{i=1}^{n} [1 - P(T_{i-1}, T_i)] \div [\tau_iP(T_{i-1}, T_i)]$ \\ 
$S = \sum_{i=1}^{n} [1 - \frac{P(t, T_i)}{P(t, T_{i-1})}] \div \tau_i \frac{P(t, T_i)}{P(t, T_{i-1})}$ \\ 
$S = \sum_{i=1}^{n} [1 - \frac{P(t, T_i)}{P(t, T_{i-1})}] \div \tau_i \frac{P(t, T_i)}{P(t, T_{i-1})} \cdot \frac{P(t, T_{i-1})}{P(t, T_{i-1})}$ \\ 
$S = \sum_{i=1}^{n} [P(t, T_{i-1}) - P(t, T_i)] \div \tau_i P(t, T_i)  $ \\ 
$S = \frac{P(t, T_0) - P(t, 1) + P(t, 1) - \dots + P(t, T_n)}{\sum_{i=1}^{n} \tau_i P(t, T_i)}  $ \\ 
$S = \frac{P(t, T_0) - P(t, T_n)}{\sum_{i=1}^{n} \tau_i P(t, T_i)} $ \\
as we sought to show.

}

\end{document}
